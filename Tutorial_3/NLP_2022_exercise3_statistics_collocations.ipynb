{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1c5cv2w2ImO"
      },
      "source": [
        "# Practical exercise 1 - Tokenizing with NLTK/SoMaJo; the distribution of tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjMLbnh61qG4"
      },
      "source": [
        "# 1. Data preparation\n",
        "\n",
        "In this exercise we use a dataset described in the following paper:\n",
        "\n",
        "> Schmidt, T., Hartl, P., Ramsauer, D., Fischer, T., Hilzenthaler, A. & Wolff, C. (2020). Acquisition and Analysis of a Meme Corpus to Investigate Web Culture. In 15th Annual International Conference of the Alliance of Digital Humanities Organizations, DH 2020, Conference Abstracts. Ottawa, Canada.\n",
        "\n",
        "\n",
        "The dataset can be found on https://github.com/lauchblatt/Memes_DH2020. The following code downloads it automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3j0JhDC53tj",
        "outputId": "64a7350c-c554-469c-d0a5-270cb1dd1b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "read 6906 meme texts; 585405 characters in total\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import codecs\n",
        "import urllib.request\n",
        "\n",
        "data_url = 'https://raw.githubusercontent.com/lauchblatt/Memes_DH2020/main/Meme_Corpus%20-%20memes.csv'\n",
        "\n",
        "texts = []\n",
        "with urllib.request.urlopen(data_url) as csvfile:\n",
        "    reader = csv.DictReader(codecs.iterdecode(csvfile, 'utf-8'))\n",
        "    for row in reader:\n",
        "        texts.append(row['text'])\n",
        "\n",
        "# remove memes without text\n",
        "texts = [text for text in texts if text!=\"NA\" and text.strip()]\n",
        "\n",
        "print(f'read {len(texts)} meme texts; {sum([len(text) for text in texts])} characters in total')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFq3un66D_lY"
      },
      "source": [
        "Let us look at a few entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVCK2k4mDhoB",
        "outputId": "9aa17edf-4833-4f6c-da9d-711c9504e64e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yo dawg we heard yo like multilasers so\n",
            "we put multilasers on yo multilasers so\n",
            "you can fire your multilasers while you fire\n",
            "your multilasers!!!!\n",
            "\n",
            "###\n",
            "mainf.cpp\n",
            "50 DALl5 HEARD SOU LKE\n",
            "OnPuTING.\n",
            "#include\n",
            "<iostream>\n",
            "#include <stdlib.h>\n",
            "using namespace std;\n",
            "int factorial(int i)\n",
            "if (1-0) return 1;\n",
            "if (i>0) return i*factorial(i-1)\n",
            "SO IPUTA FUNCTION SOUR FUNETION\n",
            "SO YOU CRN CORPUTE邑HILE yUU CORPUTE.\n",
            "Wİ\n",
            "\n",
            "###\n",
            "F SEEN C\n",
            "\n",
            "###\n",
            "YO DAWG\n",
            "SO I HEARD YOU LIKE\n",
            "VIDEO GAMES\n",
            "GAME\n",
            "Ou\n",
            "陳\n",
            ".\n",
            "12\n",
            "3\n",
            "\n",
            "###\n",
            "BABY S\n",
            "FRST\n",
            "IS PREGNANT\n",
            "TOO!\n",
            "\n",
            "###\n",
            "LOOONG\n",
            "Click to View\n",
            "\n",
            "###\n",
            "YO DAWG WE HERD YOU LIKE ARROWS SO WE PUT AN ARROW IN YO KNEE SO\n",
            "YOU CAN STOP BEING AND ADVENTURER WHILE U STOP ADVENTURING\n",
            "\n",
            "###\n",
            "Why Kzibit says \"yo dawg\"\n",
            "ALIENSH\n",
            "HD\n",
            "HISTORY.COM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('\\n###\\n'.join(texts[4022:4030]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJx5aDt-5f5c"
      },
      "source": [
        "Note: this is very strange data but should suffice for this exercise!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmpxRhaV_D2b"
      },
      "source": [
        "# 2. Data processing\n",
        "\n",
        "## Tokenizing with SoMaJo\n",
        "\n",
        "We can assume that every meme consists of one \"sentence\". To further split these into single words we have to tokenize the data.\n",
        "\n",
        "We can use the SoMaJo tokenizer which was developed especially for social media data and is easy to use.\n",
        "\n",
        "https://github.com/tsproisl/SoMaJo\n",
        "\n",
        "more info on the system: https://www.aclweb.org/anthology/W16-2607.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w5I-4DPBc4k",
        "outputId": "88de9a6c-36bf-4d09-b3e3-80b4901c434f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: SoMaJo in /home/joji/.local/lib/python3.10/site-packages (2.2.1)\n",
            "Requirement already satisfied: regex>=2019.02.18 in /home/joji/.local/lib/python3.10/site-packages (from SoMaJo) (2022.3.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install SoMaJo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V2ICBAIdBo9l"
      },
      "outputs": [],
      "source": [
        "from somajo import SoMaJo\n",
        "\n",
        "somajo_tokenizer = SoMaJo(language=\"en_PTB\",\n",
        "                          split_camel_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E-45d8AcCM9l"
      },
      "outputs": [],
      "source": [
        "data_tok = []\n",
        "for sentence in somajo_tokenizer.tokenize_text(texts):\n",
        "    data_tok.append([token.text for token in sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykl-epPuEU5T",
        "outputId": "6df57d88-2a0f-4b0f-f820-6f44cfbbe725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['MAUI']\n",
            "['Hot', 'Berks', '524', '755', 'Berks', '1084', '1266', '182', 'ratio', 'of', 'diff', '0.787878788', '0.9', '0.8', '0.7', '0.6', '0.5', 'Nostril', '(', 'L', ')', 'Nostril', '(', 'R', ')', '1127', '565', '697', '132', 'ratio', 'of', 'diff', 'y', '-0', '.', '0133x', '0.7671', 'R2', '0.2015', 'diff', '93', '0.704545455', 'Mouth', '(', 'L', 'Mouth', '(', 'Ri', ')', '546', '720', '174', '1110', 'Seriesi', 'Linear', '(', 'Series1', ')', '1235', 'ratio', 'of', 'diff', 'diff', '0.718390805', '0.3', '0.2', '0.1', 'Chin', 'forehead', '669', '39', '630', '568', '83', 'ratio', 'of', 'diff', 'clifi', '0.76984127', '4', '6', '8', '10', 'Mandible', '(', 'L', ')', 'Mandible', '(', 'Ri', '409', '1023', '1339', '316', 'ratio', 'of', 'diff', '0.718181818', '849', 'cliff', 'CONCLUSION', ':', 'PLAUSIBLE', 'Nose', '(', 'Top', ')', '288', '285', '368', '83', 'ratio', 'of', 'diff', '0.546052632', 'Nose', '(', 'Bottom', 'diff', '152', 'Eye', 'Width', '(', 'L', ')', 'X1', 'X2', 'NEA', '572', '98', '474', '1056', '1125', '69', 'ratio', 'of', 'diff', 'clifi', '0.704081633', 'Eye', 'Width', 'X1', 'X2', '(', 'R', ')', '707', '814', '107', '1230', '1306', '76', 'ratio', 'of', 'diff', '0.710280374', 'clifi']\n",
            "['ERMAHGERD', 'M', 'HOT', '.']\n",
            "['GERSBERMS', 'MAH', 'FRAVRIT', 'BERKS']\n",
            "['ERMAHGERD', 'MILK', 'BONE', 'MERLKBEHRNS', 'LARGE']\n",
            "['ERMAHGERD', 'S', 'K', 'LERNERD', 'SKERNERD']\n"
          ]
        }
      ],
      "source": [
        "print(data_tok[0])\n",
        "print(data_tok[1])\n",
        "print(data_tok[2])\n",
        "print(data_tok[3])\n",
        "print(data_tok[4])\n",
        "print(data_tok[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtv3RZCw7MV5"
      },
      "source": [
        "## Further data processing\n",
        "\n",
        "For this kind of data, lower-casing everything seems to make sense. In general: this process deletes information and also sometimes meaning; e.g. Apple (company) and apple (fruit) can not be destinguished if we ignore case. However, generalization increases. Thus, you should take this decision conciously and be aware of its effects! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oaiZupzmHS4b"
      },
      "outputs": [],
      "source": [
        "data_tok = [[token.lower() for token in sentence] for sentence in data_tok]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ohZEZr9CDr",
        "outputId": "bf4ba516-f858-4373-d3de-c423ca9801c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['maui']\n",
            "['hot', 'berks', '524', '755', 'berks', '1084', '1266', '182', 'ratio', 'of', 'diff', '0.787878788', '0.9', '0.8', '0.7', '0.6', '0.5', 'nostril', '(', 'l', ')', 'nostril', '(', 'r', ')', '1127', '565', '697', '132', 'ratio', 'of', 'diff', 'y', '-0', '.', '0133x', '0.7671', 'r2', '0.2015', 'diff', '93', '0.704545455', 'mouth', '(', 'l', 'mouth', '(', 'ri', ')', '546', '720', '174', '1110', 'seriesi', 'linear', '(', 'series1', ')', '1235', 'ratio', 'of', 'diff', 'diff', '0.718390805', '0.3', '0.2', '0.1', 'chin', 'forehead', '669', '39', '630', '568', '83', 'ratio', 'of', 'diff', 'clifi', '0.76984127', '4', '6', '8', '10', 'mandible', '(', 'l', ')', 'mandible', '(', 'ri', '409', '1023', '1339', '316', 'ratio', 'of', 'diff', '0.718181818', '849', 'cliff', 'conclusion', ':', 'plausible', 'nose', '(', 'top', ')', '288', '285', '368', '83', 'ratio', 'of', 'diff', '0.546052632', 'nose', '(', 'bottom', 'diff', '152', 'eye', 'width', '(', 'l', ')', 'x1', 'x2', 'nea', '572', '98', '474', '1056', '1125', '69', 'ratio', 'of', 'diff', 'clifi', '0.704081633', 'eye', 'width', 'x1', 'x2', '(', 'r', ')', '707', '814', '107', '1230', '1306', '76', 'ratio', 'of', 'diff', '0.710280374', 'clifi']\n",
            "['ermahgerd', 'm', 'hot', '.']\n",
            "['gersberms', 'mah', 'fravrit', 'berks']\n",
            "['ermahgerd', 'milk', 'bone', 'merlkbehrns', 'large']\n",
            "['ermahgerd', 's', 'k', 'lernerd', 'skernerd']\n"
          ]
        }
      ],
      "source": [
        "print(data_tok[0])\n",
        "print(data_tok[1])\n",
        "print(data_tok[2])\n",
        "print(data_tok[3])\n",
        "print(data_tok[4])\n",
        "print(data_tok[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc74x-tZ_LH0"
      },
      "source": [
        "# 3. Corpus statistics\n",
        "\n",
        "We will use the term \"frequency\" of a word type to express the absolute number of times this word occurs (in any context) in our corpus.\n",
        "\n",
        "Please note the terminological distinction:<br>\n",
        "**token**: Word form occuring in a text. The sentence \"This is it, is it?\" has 7 tokens \\['This', 'is', 'it', ',', 'is', 'it', '?'\\].<br>\n",
        "**type**: Unique word form in a text. The sentence \"This is it, is it?\" has 5 types {',', '?', 'This', 'is', 'it'}<br>\n",
        "A language/vocabulary consists of several word types; a corpus consists of tokens (which are mentions/occurrences of these types)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PFfJ4gYWL6Qs"
      },
      "outputs": [],
      "source": [
        "# count words and their frequencies\n",
        "from collections import Counter\n",
        "\n",
        "sentences = data_tok\n",
        "\n",
        "words = Counter(word for sentence in sentences for word in sentence)\n",
        "# Note: \"words\" now contains a mapping of words to their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yNUCGoQMFIH",
        "outputId": "efe5de34-940b-4f81-a168-49479e392a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of types (unique words): 20152\n",
            "Total number of tokens: 112846\n",
            "Number of types with frequency of occurrence 1: 12674\n",
            "Frequency of token \"man\": 87\n",
            "Frequency of token \"woman\": 34\n",
            "Frequency of token \"computing\": 0\n",
            "Frequency of token \"meaning\": 14\n",
            "Frequency of token \"!\": 704\n",
            "Frequency of token \"?\": 1060\n"
          ]
        }
      ],
      "source": [
        "# total number of types in the corpus\n",
        "print(f'Total number of types (unique words): {len(words)}')\n",
        "unique_words = len(words)\n",
        "\n",
        "# total number of tokens in the corpus\n",
        "print(f'Total number of tokens: {sum(words.values())}')\n",
        "\n",
        "# how many words occur only once?\n",
        "print(f'Number of types with frequency of occurrence 1: {len([True for word in words if words[word] == 1])}')\n",
        "\n",
        "# show the frequency of some words\n",
        "for word in ('man', 'woman', 'computing', 'meaning', '!', '?'):\n",
        "    print(f'Frequency of token \"{word}\": {words[word]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHyrQuM5Mnoz",
        "outputId": "a938036c-7d4d-429f-f28b-39c3eb7c635d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the most frequent words:\n",
            "['the', ',', 'you', '.', 'a', 'to', 'i', 'of', 'in', '?', 'and', 'is', 'so', 'my', 'it', 'like', 'yo', 'your', '!', 'that']\n",
            "\n",
            "some infrequent words:\n",
            "['prョ', 'bigbag', 'ouu', 'favre', 'webb', 'basetgod', 'asedegod', 'lurkda', 'ank', 'manter']\n"
          ]
        }
      ],
      "source": [
        "sorted_words = sorted(words, key=lambda word: words[word], reverse=True)\n",
        "\n",
        "print('the most frequent words:')\n",
        "print(sorted_words[:20])\n",
        "\n",
        "print('\\nsome infrequent words:')\n",
        "print(sorted_words[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CPgFbGDGQbBi"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "### EXERCISE (see tasks at the end of the notebook) ###\n",
        "#######################################################\n",
        "\n",
        "# You should assign each word a rank according to the sorting by its frequency (i.e. the most \n",
        "# frequent word gets rank 1, the 2nd most frequent word gets rank 2, etc.).\n",
        "# The \"ranks\" dictionary should map each word to its frequency rank.\n",
        "# print(len(sorted_words))\n",
        "\n",
        "ranks = {}\n",
        "for item in range(0, len(sorted_words)):\n",
        "    ranks[str(sorted_words[item])] = item + 1\n",
        "\n",
        "# print(sorted_words[10])\n",
        "\n",
        "# print(ranks['and'])\n",
        "\n",
        "# print(words[','])\n",
        "\n",
        "# Assign each word rank the word frequency (i.e., for example, if the word on rank 10 (= the 10th \n",
        "# most frequent word) occurs 500 times, the resulting dictionary should map 10 to 500.) \n",
        "# The \"frequency_ranks\" dictionary should save a mapping from ranks to frequencies.\n",
        "frequency_ranks = {}\n",
        "\n",
        "for j in words:\n",
        "    frequency_ranks[ranks[j]] = words[j]\n",
        "\n",
        "# print(frequency_ranks[2])\n",
        "# print(sorted(frequency_ranks.items()))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_pr3Gi8O3qg"
      },
      "source": [
        "# 4. Plotting Word Distribution\n",
        "\n",
        "Zipf's law states that: \\begin{equation}\\textit{occurrence_probability}(\\textit{word}) = \\frac{c}{\\text{rank}(\\textit{word})}\\end{equation}\n",
        "In other words: the occurrence probability of a word is inversely proportional to its frequency rank (with a corpus specific constant c).\n",
        "\n",
        "We can compute the occurrence probability of a word based on corpus data as follows:\n",
        "\\begin{equation}\n",
        "    \\textit{occurrence_probability}(\\textit{word}) = \\frac{\\textit{frequency of occurrence}(\\textit{word})}{\\textit{number of all words}}\n",
        "\\end{equation}\n",
        "For example, when a word occurs 20 times in a corpus of 100 tokens, its occurrence_probability is $0.2$.\n",
        "\n",
        "Above we calculated the frequency of occurrence of each word in our data. We now want to plot this value against the rank using Zipf's law and the formulae above.\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\textit{frequency of occurrence}(\\textit{word})}{\\textit{number of all words}} = \\frac{c}{\\text{rank}(\\textit{word})}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\textit{frequency of occurrence}(\\textit{word}) = \\frac{c * \\textit{number of all words}}{\\text{rank}(\\textit{word})}\n",
        "\\end{equation}\n",
        "\n",
        "Thus, if we want to plot the frequency on the y-axis, for any given rank $x$, the plot should display:\n",
        "\\begin{equation}\n",
        "f(x) = y = \\frac{c * \\textit{number of all words}}{x}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "9RAAfh2_O_R_",
        "outputId": "39d9f536-826b-44cb-909b-d4e2d71c72ee"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# set the constant c to some value for now\n",
        "c = 0.5\n",
        "\n",
        "# x-axis range:\n",
        "n_minimum = 3\n",
        "n_limit = 500\n",
        "\n",
        "# plot Zipf\n",
        "x_zipf = np.array(range(n_minimum, n_limit + 1))\n",
        "y_zipf = c * sum(words.values())/x_zipf  # this is the last formula above\n",
        "plt.plot(x_zipf, y_zipf, label='Zipf')\n",
        "# Note: this is what Zipf's law claims - we did not test it with our data yet.\n",
        "\n",
        "# here we plot the meme data (using the values from the cell above)\n",
        "if frequency_ranks:\n",
        "    lists = sorted(frequency_ranks.items())\n",
        "    x, y = zip(*lists)\n",
        "    plt.plot(x[n_minimum:n_limit], y[n_minimum:n_limit], label='Meme data')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLTI-vUSEE7E"
      },
      "source": [
        "#5. Collocations\n",
        "\n",
        "A collocation is \"a combination of words in a language that happens very often and more frequently than would happen by chance\".<br>\n",
        "These combinations are especially meaningful; there usually is a strong connection between the words; the words in combination often lead a new combined meaning; strong collocations can be considered lexical items.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNa3XLGFkF3"
      },
      "source": [
        "## 5.1 Co-occuring word pairs\n",
        "\n",
        "We only have the frequencies of individual words so far.\n",
        "\n",
        "To compute collocation measure we need frequencies of co-occurring word pairs.\n",
        "\n",
        "For example, the tokenized sentence ['This', 'is', 'it', ',', 'is', 'it', '?']. has the following co-occuring word pairs:\n",
        "- ('This', 'is') frequency 1\n",
        "- ('is', 'it') frequency 2\n",
        "- ('it', ',') frequency 1\n",
        "- (',', 'is') frequency 1\n",
        "- ('it', '?') frequency 1\n",
        "\n",
        "Note that the sentence has 7 tokens, thus, 6 co-occurring word pairs (also known as bigrams), however, one of those occurs twice.\n",
        "\n",
        "We now count these for our entire corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B_KZ-Cw2EEJs"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_word_pairs = Counter((word, sentence[index+1])\n",
        "                         for sentence in sentences\n",
        "                         for index, word in enumerate(sentence)\n",
        "                         if index+1 < len(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1IjS4NYF5BC",
        "outputId": "798857a8-9891-4f84-e8ca-153b9ae9f517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 10 most frequent word pairs:\n",
            "[(('yo', 'dawg'), 390), (('you', 'like'), 325), (('you', 'can'), 262), (('so', 'we'), 260), (('so', 'you'), 247), ((',', 'i'), 222), (('dat', 'ass'), 216), (('while', 'you'), 201), (('heard', 'you'), 196), (('we', 'put'), 193)]\n",
            "\n",
            "The number of unique word pairs: 64778\n",
            "\n",
            "The number of unique word pairs with a frequency greater than 1:\n",
            "51535\n"
          ]
        }
      ],
      "source": [
        "# let us look at some\n",
        "print('The 10 most frequent word pairs:')\n",
        "print(sorted(all_word_pairs.items(), key=lambda pair: pair[1], reverse=True)[:10])\n",
        "\n",
        "print(f'\\nThe number of unique word pairs: {len(all_word_pairs)}')\n",
        "\n",
        "print('\\nThe number of unique word pairs with a frequency greater than 1:')\n",
        "print(len([pair for pair, frequency in all_word_pairs.items() if frequency > 1]))\n",
        "unique_word_pairs = [pair for pair, frequency in all_word_pairs.items() if frequency == 1]\n",
        "# print(len(unique_word_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq_UG2l-G0ds",
        "outputId": "b43a7a63-7d51-4125-d834-ead3663d496d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of remaining word pairs: 1949\n"
          ]
        }
      ],
      "source": [
        "# to make it computationally feasible, only analyze word pairs with freq > some threshold\n",
        "# you might have to increase this value if running the cells in 5.2 take too long to finish\n",
        "threshold = 5\n",
        "word_pairs = {word_pair: frequency for word_pair, frequency in all_word_pairs.items() \n",
        "              if frequency >= threshold}\n",
        "print(f'number of remaining word pairs: {len(word_pairs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJJNyUdMHV9M"
      },
      "source": [
        "## 5.2 Collocation measures\n",
        "\n",
        "Above we actually used the most basic collocation measure: the frequency $o_{11}$ of the co-occurring word pair.\n",
        "\n",
        "Now we will compute the entire contingency table for each of the co-occuring word pairs (this might take a few seconds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o_epuAMaHVn9"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "#[(('yo', 'dawg'), 390), (('you', 'like'), 325), (('you', 'can'), 262), (('so', 'we'), 260), (('so', 'you'), 247), ((',', 'i'), 222), (('dat', 'ass'), 216), (('while', 'you'), 201), (('heard', 'you'), 196), (('we', 'put'), 193)]\n",
        "\n",
        "# print(len(word_pairs))\n",
        "o11 = word_pairs\n",
        "o12 = defaultdict(int)\n",
        "o21 = defaultdict(int)\n",
        "o22 = defaultdict(int)\n",
        "\n",
        "for word_pair in word_pairs:\n",
        "    word1, word2 = word_pair\n",
        "\n",
        "    for other_word_pair in word_pairs:\n",
        "        other_word1, other_word2 = other_word_pair\n",
        "\n",
        "        if word1 == other_word1:\n",
        "            if word2 != other_word2:\n",
        "                o12[word_pair] += word_pairs[other_word_pair]\n",
        "                # print(o12)\n",
        "            else:\n",
        "                # we already have this case in word_pairs\n",
        "                pass\n",
        "        else:\n",
        "            if word2 == other_word2:\n",
        "                o21[word_pair] += word_pairs[other_word_pair]\n",
        "            else:\n",
        "                o22[word_pair] += word_pairs[other_word_pair]\n",
        "\n",
        "# set min value to 1\n",
        "for pair in word_pair:\n",
        "    for cell in (o12, o21, o22):\n",
        "        if not cell[word_pair]:\n",
        "            cell[word_pair] = 1\n",
        "\n",
        "contingency_tables = {'o11': o11, 'o12': o12, 'o21': o21, 'o22': o22}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jJy5eIfuH1OU"
      },
      "outputs": [],
      "source": [
        "# A function to print highest ranked collocations, using a given collocation measure to compute ranking\n",
        "\n",
        "def print_highest_ranked_collocations(measure, top=20, tables=contingency_tables):\n",
        "    for pair in sorted(tables['o11'], key=lambda word_pair: measure(word_pair, tables), reverse=True)[:top]:\n",
        "        print((pair, tables['o11'][pair]))\n",
        "\n",
        "def print_highest_ranked_collocations_noPrint(measure, top=20, tables=contingency_tables):\n",
        "    tupList = []\n",
        "    for pair in sorted(tables['o11'], key=lambda word_pair: measure(word_pair, tables), reverse=True)[:top]:\n",
        "        # print((pair, tables['o11'][pair]))\n",
        "        tupList.append(pair)\n",
        "    return tupList\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvt0uQxgIQlS",
        "outputId": "f62f7b8b-3f91-4d9f-8520-38e166d2b7b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(('yo', 'dawg'), 390)\n",
            "(('you', 'like'), 325)\n",
            "(('you', 'can'), 262)\n",
            "(('so', 'we'), 260)\n",
            "(('so', 'you'), 247)\n",
            "((',', 'i'), 222)\n",
            "(('dat', 'ass'), 216)\n",
            "(('while', 'you'), 201)\n",
            "(('heard', 'you'), 196)\n",
            "(('we', 'put'), 193)\n",
            "(('in', 'the'), 189)\n",
            "(('of', 'the'), 185)\n",
            "(('put', 'a'), 173)\n",
            "(('i', 'heard'), 148)\n",
            "((',', 'so'), 146)\n",
            "(('dawg', ','), 142)\n",
            "(('i', \"'m\"), 138)\n",
            "(('in', 'your'), 135)\n",
            "(('dawg', 'i'), 135)\n",
            "(('i', 'herd'), 127)\n"
          ]
        }
      ],
      "source": [
        "def frequency(word_pair, tables):\n",
        "    pair_o11 = tables['o11'][word_pair]\n",
        "    return pair_o11\n",
        "\n",
        "print_highest_ranked_collocations(frequency, top=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJUmsfc9H7gj",
        "outputId": "6b8dcd8f-c063-4208-af64-2013a0c66c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(('fall', 'asleep'), 5)\n",
            "(('days', 'later'), 5)\n",
            "(('forever', 'alone'), 5)\n",
            "(('little', 'pony'), 5)\n",
            "(('best', 'friend'), 5)\n",
            "(('birthday', 'party'), 5)\n",
            "(('nice', 'gane'), 5)\n",
            "(('runs', 'marathon'), 5)\n",
            "(('aliensh', 'hd'), 5)\n",
            "(('ze', 'urger.com'), 5)\n",
            "(('tap', '*'), 5)\n",
            "(('onion', 'ring'), 5)\n",
            "(('profile', 'pictures'), 5)\n",
            "(('highest', 'place'), 5)\n",
            "(('something', 'sharp'), 5)\n",
            "(('page', 'contents'), 5)\n",
            "(('contents', 'featured'), 5)\n",
            "(('current', 'events'), 5)\n",
            "(('events', 'random'), 5)\n",
            "(('random', 'article'), 5)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# mutual information\n",
        "def mi(word_pair, tables):\n",
        "    # print('this is my word pair',word_pair)\n",
        "    pair_o11 = tables['o11'][word_pair]\n",
        "    pair_o12 = tables['o12'][word_pair]\n",
        "    pair_o21 = tables['o21'][word_pair]\n",
        "    pair_o22 = tables['o22'][word_pair]\n",
        "    \n",
        "    pair_R1 = pair_o11 + pair_o12\n",
        "    pair_C1 = pair_o11 + pair_o21\n",
        "    pair_N = pair_o11 + pair_o12 + pair_o21 + pair_o22\n",
        "    pair_e11 = pair_R1 * pair_C1 / float(pair_N)\n",
        "\n",
        "    val1 = math.log(pair_o11/pair_e11)\n",
        "    # print('this is val1', val1)\n",
        "    \n",
        "    return val1\n",
        "\n",
        "# print_highest_ranked_collocations(mi, top=20)\n",
        "result = print_highest_ranked_collocations(mi, top=20)\n",
        "\n",
        "# print(len(result))\n",
        "# print(result)\n",
        "# print((tupList))\n",
        "# print(len(tupList))\n",
        "# print(result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSoa6Rs8-79i"
      },
      "source": [
        "# 6. Exercises\n",
        "\n",
        "1.   Plotting your distribution\n",
        "\n",
        "    1.   Assign each word a rank according to the sorting by its frequency (i.e. the most frequent word gets rank 1, the 2nd most frequent word gets rank 2, etc.).\n",
        "\n",
        "    2.   Assign each word rank the word frequency (i.e., for example, if the word on rank 10 (= the 10th most frequent word) occurs 500 times, the resulting dictionary should map 10 to 500.) You should save the result in the variable 'frequency_ranks', see above.\n",
        "\n",
        "    3.   Plot your distribution together with Zipf's Law (if you defined the variable 'frequency_ranks' correctly, the code in 4. should do that). Modify the constant 'c' so the Zipf-plot fits to your data (approximately). You might also have to modify n_minimum and n_limit slightly so you can see it better.\n",
        "\n",
        "2.   Collocations\n",
        "\n",
        "    1.   Compare the number of unique word pairs to the number of unique words in our data. What do you observe? Is this expected? Why?\n",
        "    2.   Would you expect the distribution of unique word pairs also to follow Zipf’s law? Why (not)?\n",
        "    3.   Look at the top results extracted using the frequency measure. Do you think the definition of a collocation holds for these word pairs? Are these really collocations? Why (not)? What are issues when using just the frequency of word pairs as collocation measure?\n",
        "    4.   **(This is the main task of this exercise!)** Familiarize yourself with the language in this meme dataset. Write down 10 collocations for this data, i.e. pairs of words which you think are very strongly connected here (they do not really occur in other context and have a special meaning together). Implement a few (at least 5 in total) other collocation measures (http://collocations.de/AM/index.html). Which of these measures predicts the most of your 10 collocations in its top 100 results?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gTzvpnPA_C5S"
      },
      "outputs": [],
      "source": [
        "# Question 1.1, 1.2 and 1.3 are satisfied above in the given sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51535\n",
            "20152\n"
          ]
        }
      ],
      "source": [
        "# 2.1\n",
        "\n",
        "print(len(unique_word_pairs))\n",
        "# print(unique_word_pairs)\n",
        "print(unique_words)\n",
        "\n",
        "# Question 1\n",
        "# This is expected since unique word pair increase the complexity of an item (collocation) which may be used \n",
        "# in a specific context hence each word pair exists as an entity which naturally is supposed to return a higher \n",
        "# number since we are cosidering the whole pair and now individual words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2\n",
        "#Yes, since some collocations are very frequently occuring which have a very clear meaning hence in the meme world they will appear a lot more times \n",
        "#than other collocation pairs. Hence, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.3\n",
        "# No, as most of them do not seem to be valid collocation pair. Hence, with only the frequency measure we do not receive all valid colloction result.\n",
        "# Result contain meangless pairs and it could be also based on symbols and puncation which do not hold a clear meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecting collocations\n",
        "# myten = sorted(all_word_pairs.items(), key=lambda pair: pair[1], reverse=True)[:10]\n",
        "myten = [('yo', 'dawg'), ('you', 'like'), ('so', 'we'), ('yo', 'ass'), ('not','simply'), ('in', 'yo'), ('what', 'if'), ('put', 'some'), ('herd', 'you'), ('so', 'we'), ('in', 'yo'), ('yo', 'mama')]\n",
        "# myten = [x[0] for x in myten]\n",
        "# print(myten)\n",
        "\n",
        "def compare_collocations(lst1, lst2):\n",
        "    x = 0\n",
        "    # print('im here')\n",
        "    for first in lst1:\n",
        "        # print('im here 1')\n",
        "        for second in lst2:\n",
        "            # print('im here 2')\n",
        "            if first == second:\n",
        "                # print('im here 3')\n",
        "                x += 1\n",
        "                print('Collocation {} exists in the results from this measure'.format(first))\n",
        "            else:\n",
        "                pass\n",
        "    print('number of matching collcoations', x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collocation ('yo', 'dawg') exists in the results from this measure\n",
            "Collocation ('you', 'like') exists in the results from this measure\n",
            "Collocation ('so', 'we') exists in the results from this measure\n",
            "Collocation ('so', 'we') exists in the results from this measure\n",
            "Collocation ('herd', 'you') exists in the results from this measure\n",
            "Collocation ('in', 'yo') exists in the results from this measure\n",
            "Collocation ('in', 'yo') exists in the results from this measure\n",
            "number of matching collcoations 7\n"
          ]
        }
      ],
      "source": [
        "# Measure 1\n",
        "\n",
        "#asymptomatic measure\n",
        "def tscore(word_pair, tables):\n",
        "    # print('this is my word pair',word_pair)\n",
        "    pair_o11 = tables['o11'][word_pair]\n",
        "    pair_o12 = tables['o12'][word_pair]\n",
        "    pair_o21 = tables['o21'][word_pair]\n",
        "    pair_o22 = tables['o22'][word_pair]\n",
        "    \n",
        "    pair_R1 = pair_o11 + pair_o12\n",
        "    pair_C1 = pair_o11 + pair_o21\n",
        "    pair_N = pair_o11 + pair_o12 + pair_o21 + pair_o22\n",
        "    pair_e11 = pair_R1 * pair_C1 / float(pair_N)\n",
        "\n",
        "    # zs = (pair_o11 - pair_e11)/(pair_e11)^1/2\n",
        "    tscore = (pair_o11 - pair_e11)/math.sqrt(pair_o11)\n",
        "    # print('this is pl', zs)\n",
        "\n",
        "    return tscore\n",
        "\n",
        "get_list = print_highest_ranked_collocations_noPrint(tscore, top=100)\n",
        "# print('printing my get list',get_list)\n",
        "\n",
        "\n",
        "\n",
        "# if get_list[0] == myten[0]:\n",
        "#     print('printing what is to be compared', get_list[0], myten[0])\n",
        "compare_collocations(get_list, myten)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of matching collcoations 0\n"
          ]
        }
      ],
      "source": [
        "# Measure 2\n",
        "\n",
        "#point estimates\n",
        "import math\n",
        "def oddsratiodisc(word_pair, tables):\n",
        "    # print('this is my word pair',word_pair)\n",
        "    pair_o11 = tables['o11'][word_pair]\n",
        "    pair_o12 = tables['o12'][word_pair]\n",
        "    pair_o21 = tables['o21'][word_pair]\n",
        "    pair_o22 = tables['o22'][word_pair]\n",
        "    \n",
        "    pair_R1 = pair_o11 + pair_o12\n",
        "    pair_C1 = pair_o11 + pair_o21\n",
        "    pair_N = pair_o11 + pair_o12 + pair_o21 + pair_o22\n",
        "    pair_e11 = pair_R1 * pair_C1 / float(pair_N)\n",
        "\n",
        "    # zs = (pair_o11 - pair_e11)/(pair_e11)^1/2\n",
        "    \n",
        "    # print('o12', pair_o12)\n",
        "    # print('o21', pair_o21)\n",
        "    oddsratiodisc = math.log(((pair_o11+0.5)*(pair_o22+0.5))/((pair_o12+0.5)*(pair_o21+0.5)))\n",
        "    # print('this is pl', zs)\n",
        "\n",
        "    return oddsratiodisc\n",
        "\n",
        "get_list = print_highest_ranked_collocations_noPrint(oddsratiodisc, top=100)\n",
        "\n",
        "n_items = compare_collocations(get_list, myten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of matching collcoations 0\n"
          ]
        }
      ],
      "source": [
        "# Measure 3\n",
        "\n",
        "#heuristic measure\n",
        "def heuristics(word_pair, tables):\n",
        "    # print('this is my word pair',word_pair)\n",
        "    pair_o11 = tables['o11'][word_pair]\n",
        "    pair_o12 = tables['o12'][word_pair]\n",
        "    pair_o21 = tables['o21'][word_pair]\n",
        "    pair_o22 = tables['o22'][word_pair]\n",
        "    \n",
        "    pair_R1 = pair_o11 + pair_o12\n",
        "    pair_C1 = pair_o11 + pair_o21\n",
        "    pair_N = pair_o11 + pair_o12 + pair_o21 + pair_o22\n",
        "    pair_e11 = pair_R1 * pair_C1 / float(pair_N)\n",
        "\n",
        "    # zs = (pair_o11 - pair_e11)/(pair_e11)^1/2\n",
        "    \n",
        "    # print('o12', pair_o12)\n",
        "    # print('o21', pair_o21)\n",
        "    heu = math.log(pair_o11**2/pair_e11)\n",
        "    # print('this is pl', zs)\n",
        "\n",
        "    return heu\n",
        "\n",
        "get_list = print_highest_ranked_collocations_noPrint(heuristics, top=100)\n",
        "\n",
        "n_items = compare_collocations(get_list, myten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collocation ('yo', 'dawg') exists in the results from this measure\n",
            "Collocation ('so', 'we') exists in the results from this measure\n",
            "Collocation ('so', 'we') exists in the results from this measure\n",
            "Collocation ('you', 'like') exists in the results from this measure\n",
            "Collocation ('in', 'yo') exists in the results from this measure\n",
            "Collocation ('in', 'yo') exists in the results from this measure\n",
            "Collocation ('herd', 'you') exists in the results from this measure\n",
            "Collocation ('what', 'if') exists in the results from this measure\n",
            "number of matching collcoations 8\n"
          ]
        }
      ],
      "source": [
        "# Measure 4\n",
        "\n",
        "#Measure from information theory\n",
        "\n",
        "def locmi(word_pair, tables):\n",
        "    # print('this is my word pair',word_pair)\n",
        "    pair_o11 = tables['o11'][word_pair]\n",
        "    pair_o12 = tables['o12'][word_pair]\n",
        "    pair_o21 = tables['o21'][word_pair]\n",
        "    pair_o22 = tables['o22'][word_pair]\n",
        "    \n",
        "    pair_R1 = pair_o11 + pair_o12\n",
        "    pair_C1 = pair_o11 + pair_o21\n",
        "    pair_N = pair_o11 + pair_o12 + pair_o21 + pair_o22\n",
        "    pair_e11 = pair_R1 * pair_C1 / float(pair_N)\n",
        "\n",
        "    locmi = pair_o11 * math.log(pair_o11/pair_e11)\n",
        "\n",
        "    return locmi\n",
        "\n",
        "get_list = print_highest_ranked_collocations_noPrint(locmi, top=100)\n",
        "\n",
        "n_items = compare_collocations(get_list, myten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collocation ('yo', 'dawg') exists in the results from this measure\n",
            "number of matching collcoations 1\n"
          ]
        }
      ],
      "source": [
        "# Measure 5\n",
        "\n",
        "#Measure from likelihood measure\n",
        "# poisson-stirlinglog\n",
        "\n",
        "def poStir(word_pair, tables):\n",
        "    # print('this is my word pair',word_pair)\n",
        "    pair_o11 = tables['o11'][word_pair]\n",
        "    pair_o12 = tables['o12'][word_pair]\n",
        "    pair_o21 = tables['o21'][word_pair]\n",
        "    pair_o22 = tables['o22'][word_pair]\n",
        "    \n",
        "    pair_R1 = pair_o11 + pair_o12\n",
        "    pair_C1 = pair_o11 + pair_o21\n",
        "    pair_N = pair_o11 + pair_o12 + pair_o21 + pair_o22\n",
        "    pair_e11 = pair_R1 * pair_C1 / float(pair_N)\n",
        "\n",
        "    postir = pair_o11 ** (math.log(pair_o11) - math.log(pair_e11) - 1)\n",
        "\n",
        "    return postir\n",
        "\n",
        "get_list = print_highest_ranked_collocations_noPrint(poStir, top=100)\n",
        "\n",
        "n_items = compare_collocations(get_list, myten)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP-2022_exercise3_statistics-collocations.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
