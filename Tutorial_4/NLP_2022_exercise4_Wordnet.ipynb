{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1c5cv2w2ImO"
      },
      "source": [
        "# Practical exercise 4 - experiments with Wordnet\n",
        "\n",
        "WordNet is a large digital lexicon made by hand. The kernel of WordNet are the so called synsets that can be understood as meanings. Each word belongs to one or more synsets and each synset is made up of one or more words. Semantic relations like hypernymy and hyponymy exist between synsets, not between words! Consequently, there is no such thing like synonymy in Wordnet. If two words are synonymous the will share one or several synsets.<br>\n",
        "It is possible to access Wordnet is via the web interface: http://wordnetweb.princeton.edu/perl/webwn. There we can see e.g. the synsets\n",
        "of a word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlQOh0mujYeT"
      },
      "source": [
        "# 1. WordNet in Python\n",
        "\n",
        "The NLTK package offers some easy methods to access WordNet. Before you use WordNet you have to run once the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHfttgQnjpUi",
        "outputId": "02950680-5e18-4af0-e751-1dbbe71e99ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/joji/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/joji/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVsKddF9kVRJ"
      },
      "source": [
        "How to access synsets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luBhaPNJj9NE",
        "outputId": "93b980a3-749c-4f11-97a5-0fd492b1a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synset('rock.n.01')\n",
            "Synset('rock.n.02')\n",
            "Synset('rock.n.03')\n",
            "Synset('rock.n.04')\n",
            "Synset('rock_candy.n.01')\n",
            "Synset('rock_'n'_roll.n.01')\n",
            "Synset('rock.n.07')\n",
            "Synset('rock.v.01')\n",
            "Synset('rock.v.02')\n",
            "\n",
            "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
            "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
            "[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# get synsets of a word\n",
        "synsets = wn.synsets(\"rock\")\n",
        "\n",
        "for s in synsets:\n",
        "    print(s)\n",
        "print()\n",
        "\n",
        "# use synset identifier directly\n",
        "dog = wn.synset(\"dog.n.01\")\n",
        "print(dog.hypernyms())\n",
        "print(dog.hyponyms())\n",
        "print(dog.lemmas())  # ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZPvJKVys2UW"
      },
      "source": [
        "An easy way to compute the similarity between two synsets is to measure the length of the path between the synsets in the WordNet hierarchy made up by the hypernym relations. The method path_simiarity returns 1/p where p is the length of the path between two synsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-SrM7VLrXnp",
        "outputId": "078aa484-d188-4efb-a799-12a232ca7749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between ape and monkey:  0.3333333333333333\n",
            "Similarity between ape and zoo:  0.07692307692307693\n"
          ]
        }
      ],
      "source": [
        "ape = wn.synset(\"ape.n.01\")\n",
        "monkey = wn.synset(\"monkey.n.01\")\n",
        "zoo = wn.synset(\"zoo.n.01\")\n",
        "\n",
        "print( \"Similarity between ape and monkey: \", ape.path_similarity(monkey))\n",
        "print( \"Similarity between ape and zoo: \", ape.path_similarity(zoo))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6PrfM-RtC8i"
      },
      "source": [
        "Wordnet is not completely connected. The path similarity method therefore assumes a fake root node that connect all parts. The path similarity has the problem that words are less similar if they are part of the hierarchy that is worked out in more detail. In general we would assume that the \u001bfirst divisions at the top of the hierarchy imply large semantic differences, while a division at a very deep position in the hierarchy makes only small semantic distinctions. Therefore some alternative measures have been defi\u001bned, e.g. the Wu-Palmer similarity and the Leacock-Chodorow similarity (feel free to read up on those measures)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iIAT5CasVTg",
        "outputId": "0b820964-3883-4eb5-ef8c-dea400e5c369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wu竏単almer similarity between ape and monkey:  0.9230769230769231\n",
            "Wu竏単almer similarity between ape and zoo:  0.4\n",
            "Leacock Chodorow similarity between ape and monkey:  2.538973871058276\n",
            "Leacock Chodorow similarity between ape and zoo:  1.072636802264849\n"
          ]
        }
      ],
      "source": [
        "print(\"Wu竏単almer similarity between ape and monkey: \", ape.wup_similarity(monkey))\n",
        "print(\"Wu竏単almer similarity between ape and zoo: \", ape.wup_similarity(zoo))\n",
        "\n",
        "print(\"Leacock Chodorow similarity between ape and monkey: \", ape.lch_similarity(monkey))\n",
        "print(\"Leacock Chodorow similarity between ape and zoo: \", ape.lch_similarity(zoo))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz6udeQWtsXT"
      },
      "source": [
        "Both measures give higher weight to distances between nodes that are closer to the root. However, the distance to the root is also a design decision and a number of measures try to include other information sources as well. E.g. the similarity measures of Resnik and Lin include the frequency of words in a corpus as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tTTvM1w48FN"
      },
      "source": [
        "# 2. Exercise:\n",
        "\n",
        "0. Read in the email dataset (see exercise 3). You may copy some of the code from that notebook.\n",
        "\n",
        "1. Let us investigate the coverage of this data in Wordnet:\n",
        "    - Count the unique words (types) in the data and store them in a list.\n",
        "    - How many of those items have synsets in Wordnet? (calculate a percentage value)\n",
        "    - What is the average number of synsets per type?\n",
        "\n",
        "2. Not all words have lexical meaning. We can filter certain word classes. Apply POS-tagging (for example https://www.nltk.org/book/ch05.html) to extract only nouns (just NN - not proper nouns NNP). Check the coverage in Wordnet for these nouns. How many have synsets in Wordnet? (calculate a percentage value)\n",
        "\n",
        "3. Experiments with the similarity of words:\n",
        "    - Choose 10 out of the 50 most frequent nouns from the data set (they all should have at least one synset in Wordnet).\n",
        "    - Now compute for each of the 10 words the Wu-Palmer or Leacock-Chodorow similarity to each of the other 9 words (you may use the first synset for each word for this calculation when words have multiple synsets). You might want to display the resulting numbers in a table. Which words are most similar to each other?\n",
        "    - Check for all sentences which contain the word 'Obama': How often does each of the 10 words you selected occur in these sentences? Have words with similar meaning also similar co-occurrence counts with 'Obama'?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gQIZYzIy7AHW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of email bodies: 6741\n"
          ]
        }
      ],
      "source": [
        "# 0\n",
        "\n",
        "# read the file\n",
        "texts = open('emails-body.txt').read().split('<cmail>\\n')\n",
        "# str_texts = \",\".join(texts)\n",
        "# print(type(str_texts))\n",
        "# print(str_texts)\n",
        "print(f'number of email bodies: {len(texts)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1\n",
        "# Counting words(types)\n",
        "\n",
        "from somajo import SoMaJo\n",
        "\n",
        "somajo_tokenizer = SoMaJo(language=\"en_PTB\", split_camel_case=True)\n",
        "\n",
        "# this might take a minute\n",
        "data_tok = []\n",
        "for sentence in somajo_tokenizer.tokenize_text(texts):\n",
        "    data_tok.append([token.text for token in sentence])\n",
        "# print(len(data_tok))\n",
        "# print(data_tok[200])\n",
        "\n",
        "# print(data_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of types (unique words): 37340\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# count words and their frequencies\n",
        "from collections import Counter\n",
        "\n",
        "sentences = data_tok\n",
        "# print(sentences)\n",
        "\n",
        "words = Counter(word for sentence in sentences for word in sentence)\n",
        "# Note: \"words\" now contains a mapping of words to their frequencies.\n",
        "\n",
        "# total number of types in the corpus\n",
        "print(f'Total number of types (unique words): {len(words)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of words having synsets 66.24799143010178\n"
          ]
        }
      ],
      "source": [
        "# 1.2\n",
        "# how many in 'words' have synsets in wordnet\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "wordList = list(words)\n",
        "\n",
        "cnt = 0\n",
        "totalSynsets = []\n",
        "synsetLengths = []\n",
        "for w in wordList:\n",
        "    synsets = wn.synsets(w)\n",
        "    # print('synsets is {} and len is {}'.format(synsets, len(synsets)))\n",
        "    if len(synsets) > 0:\n",
        "        synsetLengths.append(len(synsets))\n",
        "        cnt += 1\n",
        "# print(cnt)\n",
        "# print(sum(synsetLengths)/len(wordList))\n",
        "# print(len(synsetLengths))\n",
        "# print(len(wordList))\n",
        "\n",
        "print('Percentage of words having synsets', 100*(cnt/len(wordList)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average number of synsets per word is 3.0951526513122656\n"
          ]
        }
      ],
      "source": [
        "# 1.3 \n",
        "# avg number of synsets per word\n",
        "import numpy as np\n",
        "print('The average number of synsets per word is',sum(synsetLengths)/len(wordList))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wordnet coverage for nouns in % is:  81.92861159813424\n"
          ]
        }
      ],
      "source": [
        "# 2\n",
        "# Not all words have lexical meaning. We can filter certain word classes. Apply POS-tagging (for example https://www.nltk.org/book/ch05.html) to extract only nouns\n",
        "#  (just NN - not proper nouns NNP). Check the coverage in Wordnet for these nouns. How many have synsets in Wordnet? (calculate a percentage value)\n",
        "import nltk\n",
        "from nltk import grammar, parse\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data_tok = [[token.lower() for token in sentence] for sentence in data_tok]\n",
        "flat_list = [item for sublist in data_tok for item in sublist]\n",
        "# print(flat_list)\n",
        "\n",
        "# print(type(data_tok[1]))\n",
        "tagged_tokens = nltk.pos_tag(flat_list)\n",
        "# print(tagged_tokens) \n",
        "\n",
        "NN_lst = []\n",
        "for ind in range(len(tagged_tokens)):\n",
        "    if tagged_tokens[ind][1] == 'NN':\n",
        "        NN_lst.append(tagged_tokens[ind][0])\n",
        "# print('nouns lst', NN_lst)\n",
        "\n",
        "ncnt = 0\n",
        "NN_synsets = []\n",
        "for n in NN_lst:\n",
        "    synsets = wn.synsets(n)\n",
        "    if len(synsets) > 0:\n",
        "        NN_synsets.append(n)\n",
        "        ncnt += 1\n",
        "\n",
        "# print(ncnt)\n",
        "print(\"Wordnet coverage for nouns in % is: \", 100*(ncnt/len(NN_lst)))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 most common ['i', 'pm', 'state', 'secretary', 'office', 'president', 'time', 'house', 'department', 'government', 'clinton', 'today', 'meeting', 'policy', 'security', 'h', 'party', 'world', 'room', 'way', 'call', 'year', 'minister', 'administration', 'tomorrow', 're', 'case', 'bill', 'day', 'week', 'staff', 'israel', 'washington', 'conference', 'part', 'work', 'health', 'country', 'agreement', 'information', 'group', 'w', 'support', 'percent', 'peace', 'speech', 'war', 'date', 'benghazi', 'afghanistan']\n",
            "\n",
            "Selected nouns:  ['president', 'staff', 'department', 'government', 'clinton', 'washington', 'work', 'country', 'minister', 'administration']\n"
          ]
        }
      ],
      "source": [
        "# 3.1\n",
        "# Instead of selecting randomly from top 50, I am taking the top 10 most common nouns that have at least 1 synsets\n",
        "import collections\n",
        "counter_NN_synsets = collections.Counter(NN_synsets)\n",
        "# print(counter_NN_lst)\n",
        "mostCommon = counter_NN_synsets.most_common(50)\n",
        "\n",
        "plain_lst = [key for key, _ in mostCommon]\n",
        "print('Top 50 most common',plain_lst)\n",
        "\n",
        "plain_lst = ['president', 'staff', 'department', 'government', 'clinton', 'washington', 'work', 'country','minister', 'administration']\n",
        "print()\n",
        "print('Selected nouns: ', plain_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "words similar to eachother as seen in this section are STAFF - DEPARTMENT and DEPARTMENT - GOVERNMENT\n",
            "\n",
            "Leacock Chodorow similarity between president and staff: 0.8649974374866046\n",
            "Leacock Chodorow similarity between president and department: 0.7472144018302211\n",
            "Leacock Chodorow similarity between president and government: 0.8649974374866046\n",
            "Leacock Chodorow similarity between president and clinton: 1.3350010667323402\n",
            "Leacock Chodorow similarity between president and washington: 0.7472144018302211\n",
            "Leacock Chodorow similarity between president and work: 0.8649974374866046\n",
            "Leacock Chodorow similarity between president and country: 0.8043728156701697\n",
            "Leacock Chodorow similarity between president and minister: 1.4403615823901665\n",
            "Leacock Chodorow similarity between president and administration: 0.8043728156701697\n",
            "Leacock Chodorow similarity between staff and president: 0.8649974374866046\n",
            "Leacock Chodorow similarity between staff and department: 1.6916760106710724\n",
            "Leacock Chodorow similarity between staff and government: 2.0281482472922856\n",
            "Leacock Chodorow similarity between staff and clinton: 0.9295359586241757\n",
            "Leacock Chodorow similarity between staff and washington: 0.8043728156701697\n",
            "Leacock Chodorow similarity between staff and work: 1.2396908869280152\n",
            "Leacock Chodorow similarity between staff and country: 1.845826690498331\n",
            "Leacock Chodorow similarity between staff and minister: 0.9985288301111273\n",
            "Leacock Chodorow similarity between staff and administration: 1.1526795099383855\n",
            "Leacock Chodorow similarity between department and president: 0.7472144018302211\n",
            "Leacock Chodorow similarity between department and staff: 1.6916760106710724\n",
            "Leacock Chodorow similarity between department and government: 1.6916760106710724\n",
            "Leacock Chodorow similarity between department and clinton: 0.8043728156701697\n",
            "Leacock Chodorow similarity between department and washington: 0.6931471805599453\n",
            "Leacock Chodorow similarity between department and work: 1.072636802264849\n",
            "Leacock Chodorow similarity between department and country: 1.845826690498331\n",
            "Leacock Chodorow similarity between department and minister: 0.8649974374866046\n",
            "Leacock Chodorow similarity between department and administration: 0.9985288301111273\n",
            "Leacock Chodorow similarity between government and president: 0.8649974374866046\n",
            "Leacock Chodorow similarity between government and staff: 2.0281482472922856\n",
            "Leacock Chodorow similarity between government and department: 1.6916760106710724\n",
            "Leacock Chodorow similarity between government and clinton: 0.9295359586241757\n",
            "Leacock Chodorow similarity between government and washington: 0.8043728156701697\n",
            "Leacock Chodorow similarity between government and work: 1.2396908869280152\n",
            "Leacock Chodorow similarity between government and country: 1.845826690498331\n",
            "Leacock Chodorow similarity between government and minister: 0.9985288301111273\n",
            "Leacock Chodorow similarity between government and administration: 1.1526795099383855\n",
            "Leacock Chodorow similarity between clinton and president: 1.3350010667323402\n",
            "Leacock Chodorow similarity between clinton and staff: 0.9295359586241757\n",
            "Leacock Chodorow similarity between clinton and department: 0.8043728156701697\n",
            "Leacock Chodorow similarity between clinton and government: 0.9295359586241757\n",
            "Leacock Chodorow similarity between clinton and washington: 0.8043728156701697\n",
            "Leacock Chodorow similarity between clinton and work: 0.9295359586241757\n",
            "Leacock Chodorow similarity between clinton and country: 0.8649974374866046\n",
            "Leacock Chodorow similarity between clinton and minister: 1.55814461804655\n",
            "Leacock Chodorow similarity between clinton and administration: 0.8649974374866046\n",
            "Leacock Chodorow similarity between washington and president: 0.7472144018302211\n",
            "Leacock Chodorow similarity between washington and staff: 0.8043728156701697\n",
            "Leacock Chodorow similarity between washington and department: 0.6931471805599453\n",
            "Leacock Chodorow similarity between washington and government: 0.8043728156701697\n",
            "Leacock Chodorow similarity between washington and clinton: 0.8043728156701697\n",
            "Leacock Chodorow similarity between washington and work: 0.8043728156701697\n",
            "Leacock Chodorow similarity between washington and country: 0.7472144018302211\n",
            "Leacock Chodorow similarity between washington and minister: 0.8649974374866046\n",
            "Leacock Chodorow similarity between washington and administration: 0.7472144018302211\n",
            "Leacock Chodorow similarity between work and president: 0.8649974374866046\n",
            "Leacock Chodorow similarity between work and staff: 1.2396908869280152\n",
            "Leacock Chodorow similarity between work and department: 1.072636802264849\n",
            "Leacock Chodorow similarity between work and government: 1.2396908869280152\n",
            "Leacock Chodorow similarity between work and clinton: 0.9295359586241757\n",
            "Leacock Chodorow similarity between work and washington: 0.8043728156701697\n",
            "Leacock Chodorow similarity between work and country: 1.1526795099383855\n",
            "Leacock Chodorow similarity between work and minister: 0.9985288301111273\n",
            "Leacock Chodorow similarity between work and administration: 1.6916760106710724\n",
            "Leacock Chodorow similarity between country and president: 0.8043728156701697\n",
            "Leacock Chodorow similarity between country and staff: 1.845826690498331\n",
            "Leacock Chodorow similarity between country and department: 1.845826690498331\n",
            "Leacock Chodorow similarity between country and government: 1.845826690498331\n",
            "Leacock Chodorow similarity between country and clinton: 0.8649974374866046\n",
            "Leacock Chodorow similarity between country and washington: 0.7472144018302211\n",
            "Leacock Chodorow similarity between country and work: 1.1526795099383855\n",
            "Leacock Chodorow similarity between country and minister: 0.9295359586241757\n",
            "Leacock Chodorow similarity between country and administration: 1.072636802264849\n",
            "Leacock Chodorow similarity between minister and president: 1.4403615823901665\n",
            "Leacock Chodorow similarity between minister and staff: 0.9985288301111273\n",
            "Leacock Chodorow similarity between minister and department: 0.8649974374866046\n",
            "Leacock Chodorow similarity between minister and government: 0.9985288301111273\n",
            "Leacock Chodorow similarity between minister and clinton: 1.55814461804655\n",
            "Leacock Chodorow similarity between minister and washington: 0.8649974374866046\n",
            "Leacock Chodorow similarity between minister and work: 0.9985288301111273\n",
            "Leacock Chodorow similarity between minister and country: 0.9295359586241757\n",
            "Leacock Chodorow similarity between minister and administration: 0.9295359586241757\n",
            "Leacock Chodorow similarity between administration and president: 0.8043728156701697\n",
            "Leacock Chodorow similarity between administration and staff: 1.1526795099383855\n",
            "Leacock Chodorow similarity between administration and department: 0.9985288301111273\n",
            "Leacock Chodorow similarity between administration and government: 1.1526795099383855\n",
            "Leacock Chodorow similarity between administration and clinton: 0.8649974374866046\n",
            "Leacock Chodorow similarity between administration and washington: 0.7472144018302211\n",
            "Leacock Chodorow similarity between administration and work: 1.6916760106710724\n",
            "Leacock Chodorow similarity between administration and country: 1.072636802264849\n",
            "Leacock Chodorow similarity between administration and minister: 0.9295359586241757\n"
          ]
        }
      ],
      "source": [
        "# 3.2 \n",
        "# Now compute for each of the 10 words the Wu-Palmer or Leacock-Chodorow similarity to each of the other 9 words\n",
        "# (you may use the first synset for each word for this calculation when words have multiple synsets).\n",
        "# You might want to display the resulting numbers in a table. Which words are most similar to each other?\n",
        "print('words similar to eachother as seen in this section are STAFF - DEPARTMENT and DEPARTMENT - GOVERNMENT') \n",
        "print()\n",
        "from nltk.corpus import wordnet as wn\n",
        "for w1 in plain_lst:\n",
        "    # print('i am here')\n",
        "    for w2 in plain_lst:\n",
        "        if w1 != w2:            \n",
        "            synsets1 = wn.synsets(w1)\n",
        "            word_a = synsets1[0]\n",
        "            synsets2 = wn.synsets(w2)\n",
        "            word_b = synsets2[0]\n",
        "            print(\"Leacock Chodorow similarity between {} and {}: {}\".format(w1, w2, word_a.lch_similarity(word_b)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.3\n",
        "# Check for all sentences which contain the word 'Obama': How often does each of the 10 words you \n",
        "# selected occur in these sentences? Have words with similar meaning also similar co-occurrence counts with 'Obama'?\n",
        "\n",
        "# print(sentences[:10])\n",
        "text_str = ''.join(texts)\n",
        "# print(text_str)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentences_1 = sent_tokenize(text_str)\n",
        "# print(sentences_1)\n",
        "# print(type(sentences_1))\n",
        "# print(len(sentences_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "obama_lst = []\n",
        "for elems in sentences_1:\n",
        "    if 'Obama' in elems:\n",
        "        obama_lst.append(elems)\n",
        "# print(len(obama_lst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "selected words:  ['president', 'staff', 'department', 'government', 'clinton', 'washington', 'work', 'country', 'minister', 'administration'] respective their counts are:  [96, 19, 3, 30, 4, 1, 38, 20, 8, 118]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "occurence_counts = [0]*10\n",
        "for p in range(len(plain_lst)):\n",
        "    for occur in obama_lst:\n",
        "        if plain_lst[p] in occur:\n",
        "            occurence_counts[p] += 1\n",
        "# print(occurence_counts, 'for selected words :', plain_lst)\n",
        "print('selected words: ', plain_lst, 'respective their counts are: ', occurence_counts)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP-2022_exercise4_Wordnet.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
