{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Based Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement a simple count based model. We will not try to optimize the model in order to get the best results possible. Instead we will try to keep things as simple as possible. Thus the main principles can become clear. \n",
    "\n",
    "We will use a mid-sized corpus as a compromise between fast processing and usefull results. \n",
    "\n",
    "In this notebook we will use German data. However, not much knowledge of German is required. And learning some new German words on the fly is not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Caution</span>**\n",
    "\n",
    "Some of the cells require quite long computation time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use a corpus of 300k sentences from wikipedia collected by the university of Leipzig. You can download the required data from http://wortschatz.uni-leipzig.de/en/download/ . The file used here is **deu_wikipedia_2016_300K-sentences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "The texts in the corpus are already split into sentences. We read thes sentence and word-tokenize each sentence. To save time we use the infected words and do not do anly lemmatization or stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "sentences = []\n",
    "\n",
    "#Caution: change the path to this file!\n",
    "source = codecs.open('/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/deu_wikipedia_2016_300K/deu_wikipedia_2016_300K-sentences.txt','r','utf8')\n",
    "for line in source:\n",
    "    nr,sent = line.split('\\t')\n",
    "    sentences.append(nltk.word_tokenize(sent.strip(),language='german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether the senteces are in the list as we expect them to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1819',\n",
       " 'wurde',\n",
       " 'daher',\n",
       " 'ein',\n",
       " 'neues',\n",
       " '„',\n",
       " 'Vereidigungsbuch',\n",
       " '“',\n",
       " 'angelegt',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[447]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors of Co-occurrence Values\n",
    "\n",
    "\n",
    "We write a function that computes vectors with co-occurrence numbers for a number of  words with a given list of 'context' words. As context words we take all words that exceed a specified minimum frequency. KCo-occurrence with rare words will hardly contribute to the comparison between context vectors. Another parameter that needs to be set is the maximum distance between two words to count as co-competition, known as the window_size size. We proceed sentence by sentence here. This means that the last word of a sentence and the first word of the next sentence are never counted as co-occurring. Note, however, that sentence bondaries are often not taken into account in such procedures. It is not clear whether this has a seriosu impact. Another detail to note is that we first calculate the window size for the competition, and then determine the relevant words in the window. Alternatively, you can first remove all irrelevant words and then determine the words within the window.\n",
    "\n",
    "In general, a larger window can compensate for a too small corpus. In addition, a smaller window captures more syntactical properties of a word, while a larger window takes into account broader semantic relationships.\n",
    "\n",
    "When all co-occurrence values are calculated, we normalize the length of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def mag(x):\n",
    "    return np.sqrt(x.dot(x))\n",
    "\n",
    "\n",
    "def makeCV(words,sentences, window = 2, minfreq = 10):\n",
    "    #first count all words\n",
    "    freq = Counter()\n",
    "    for s in sentences:\n",
    "        freq.update(s)\n",
    "        \n",
    "    #determine which words have to be used as features or context words\n",
    "    context_words = [w for w,f in freq.items() if f > minfreq]\n",
    "    \n",
    "    #we add all words to the context words, if they are not already in that list. \n",
    "    #Just for convenience to have all words in one list. \n",
    "    for w in words:\n",
    "        if w not in context_words:\n",
    "            context_words.append(w)\n",
    "    dim = len(context_words)\n",
    "    \n",
    "    #Give each context word a unique number and build dictionaries to switch between numbers and words\n",
    "    cw2nr = {}\n",
    "    for i in range(dim):\n",
    "        cw = context_words[i]\n",
    "        cw2nr[cw] = i\n",
    "              \n",
    "    w2nr = {}\n",
    "    for i in range(len(words)):\n",
    "        w = words[i]\n",
    "        w2nr[w] = i\n",
    "    \n",
    "    \n",
    "    #initialize a matrix\n",
    "    #We use  a sparse matrix class from scipy\n",
    "    matrix = sparse.lil_matrix((len(words),dim))\n",
    "    \n",
    "    #Now we start the real work. We iterate through all sentences and count the co-occurrences!\n",
    "    for s in sentences:\n",
    "        i_s = [cw2nr.get(w,-1) for w in s]\n",
    "        for i in range(len(s)):\n",
    "            w = s[i]\n",
    "            if w in words:\n",
    "                i_w = w2nr[w]\n",
    "                for j in range(max(0,i-window),min(i+window+1,len(s))):\n",
    "                    if i != j: # a word is not in its own context!\n",
    "                        i_cw = i_s[j]\n",
    "                        if i_cw > 0:\n",
    "                            matrix[i_w,i_cw] += 1\n",
    "          \n",
    "    #finally make a dictionary with vectors for each word\n",
    "    wordvectors = {}\n",
    "    for w,i_w in w2nr.items():\n",
    "        v = matrix[i_w].toarray()[0]\n",
    "        wordvectors[w] = v/mag(v)\n",
    "    return wordvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the function for a short list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_count =  makeCV(['Kloster','Kirche','Garten','Haus','Hof','Schweden','Deutschland','betrachten','anschauen','beobachten'],sentences,window=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the vectors have the same length, we can use the inner product, which is identical to the cosine, for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8824327413213885\n",
      "0.7023764205372964\n"
     ]
    }
   ],
   "source": [
    "print(vectors_count['Schweden'].dot(vectors_count['Deutschland']))\n",
    "print(vectors_count['Schweden'].dot(vectors_count['Hof']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to know, what words are most similar to a given word. To do so, we need to compare a word with each other word in the list. We use an ordered list to store the results. Since these list always sort ascending, we need to consider always the last elements of this list. Finally, we return the results in inverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect \n",
    "\n",
    "def most_similar(word,vectors,n):\n",
    "    best = []\n",
    "    vec_w = vectors[word]\n",
    "    for z in vectors:\n",
    "        if z == word:\n",
    "            continue\n",
    "        sim = vec_w.dot(vectors[z])\n",
    "\n",
    "        #we have to add this result only, if we do not yet have n results, or if the similarity is larger than the similarity with the last element in the list (actually the first since we sort ascending)\n",
    "        if len(best) < n or sim > best[0][0]:\n",
    "            bisect.insort(best,(sim,z))\n",
    "            best = best[-n:]\n",
    "       \n",
    "    return best[::-1] #present the list in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8131085029186811, 'Hof'),\n",
       " (0.6601100880142219, 'Haus'),\n",
       " (0.6391321442093744, 'Schweden')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Garten',vectors_count,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more interesing to find the most similar word if we have more words to choose from. Let us collect some mid-frequency words to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12177\n"
     ]
    }
   ],
   "source": [
    "wortfrequenz = Counter()\n",
    "\n",
    "for satz in sentences:\n",
    "    wortfrequenz.update(satz)\n",
    "\n",
    "vocabulary  = [w for w,f in wortfrequenz.items() if 30 < f < 3000]\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it takes some time to compute all vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_count = makeCV(vocabulary,sentences,window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9303729732770968, 'Park'),\n",
       " (0.8713290722481782, 'Saal'),\n",
       " (0.8638230056577244, 'Tempel'),\n",
       " (0.8631281311361956, 'Bezirk'),\n",
       " (0.8618980713054947, 'Sturm'),\n",
       " (0.8599988689679838, 'Keller'),\n",
       " (0.8571963797713257, 'Raum'),\n",
       " (0.854935433012446, 'Wald'),\n",
       " (0.8531439466922369, 'Kreis'),\n",
       " (0.8458564830673464, 'Sinn')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Garten',vectors_count,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9585602463943912, 'verstehen'),\n",
       " (0.9577321881251438, 'ermitteln'),\n",
       " (0.9575302761329436, 'schaffen'),\n",
       " (0.9566353415343976, 'bauen'),\n",
       " (0.9549942222425168, 'beobachten'),\n",
       " (0.9537790612894577, 'verwenden'),\n",
       " (0.9525533613296538, 'bringen'),\n",
       " (0.9523625237497214, 'erhöhen'),\n",
       " (0.9484461037256293, 'kämpfen'),\n",
       " (0.9478641686123344, 'retten')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('betrachten',vectors_count,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9466501415959565, 'Ungarn'),\n",
       " (0.9385311272990015, 'Polen'),\n",
       " (0.9344181199344914, 'Frankreich'),\n",
       " (0.930491672233815, 'Australien'),\n",
       " (0.9287830698511249, 'Spanien'),\n",
       " (0.9231790664595388, 'Russland'),\n",
       " (0.9228546620449983, 'Italien'),\n",
       " (0.9221253661112269, 'England'),\n",
       " (0.9194553370244143, 'Kanada'),\n",
       " (0.9179109485171332, 'Brasilien')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Schweden',vectors_count,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9483059267409419, 'Leiter'),\n",
       " (0.941059344345683, 'Chef'),\n",
       " (0.9373318244835647, 'Kommandeur'),\n",
       " (0.9343354455869818, 'Vorsitzender'),\n",
       " (0.9231897847174313, 'Präsident'),\n",
       " (0.9108755997814568, 'Mitglied'),\n",
       " (0.8921792527539255, 'Vizepräsident'),\n",
       " (0.8718734003027769, 'Vorstandsmitglied'),\n",
       " (0.8696699002069143, 'Sekretär'),\n",
       " (0.8601018930922202, 'Kommandant')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Direktor',vectors_count,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "The examples look nice, but how good are our vectors? There are a number of tests that can be done to check the quality of the vectors. One type of test is to compare the calculated similarity between two words to the similarity that subjects have given for word pairs. We can simply use the correlation to check the similarity.\n",
    "\n",
    "A dataset with similarity assessments for German word pairs is the so-called Gur350 dataset, which was developed by the working group of Prof. Dr. Iryna Gurevych at the TU Darmstadt. Further information and a link for the download can be found here: https://www.informatik.tu-darmstadt.de/ukp/research_6/data/semantic_relatedness/german_relatedness_datasets/index.en.jsp\n",
    "\n",
    "The data does not give a similarity between the words but a relationship, which is not exactly the same. For the evaluation, we only use the word pairs for which both words are contained in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "testfile = codecs.open('/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/datasets/wortpaare350.gold.pos.txt','r','utf8')\n",
    "testfile.readline()\n",
    "\n",
    "testdata = []\n",
    "missing = set()\n",
    "for line in testfile:\n",
    "    w1,w2,sim,p1,p2 = line.split(':')\n",
    "    if w1 in vocabulary and w2 in vocabulary:\n",
    "        testdata.append((w1,w2,float(sim)))\n",
    "    \n",
    "def evaluate(data,vectors):\n",
    "    gold = []\n",
    "    predicted = []\n",
    "    for v,w,sim in data:\n",
    "        pred = vectors[v].dot(vectors[w])\n",
    "\n",
    "        gold.append(sim)\n",
    "        predicted.append(pred)\n",
    "        #print(v,w,pred,sim,sep = '\\t')\n",
    "        \n",
    "    av_p = sum(predicted)/len(predicted)\n",
    "    av_g = sum(gold)/len(gold)\n",
    "    \n",
    "    cov = 0\n",
    "    var_g = 0\n",
    "    var_p = 0\n",
    "    for s,t in zip(gold,predicted):\n",
    "        cov += (s-av_g) * (t-av_p)\n",
    "        var_g += (s-av_g) * (s-av_g)\n",
    "        var_p += (t-av_p) * (t-av_p)\n",
    "        \n",
    "    return cov / math.sqrt(var_g*var_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we are using only a very small part of the test data from the Gur350 dataset for testing. Thus we cannot compare our results to official results for these data! We can of course include more words in our vocabulary of mid-frequency words, but many words simpy are not in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17255411250703448"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(testdata,vectors_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the good looking lists that we generated above, we see that the calculated similarities correlate only very weakly with the similarity judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somewhat more advanced\n",
    "\n",
    "\n",
    "Our first attempt still offers various possibilities for optimization. First of all we could not use the simple co-occurrence frequencies but the _Positive Pointwise Mutual Information_ (PPMI). The _Pointwise Mutual Information_ between two words a and b is in principle the ratio between the actual probability that they will occur together and the expected probability that they will occur if their occurrences were independent. We define $pmi(a,b) = log(\\frac{p(ab)}{p(a)p(b)})$. The ppmi(a, b) is now the pmi(a, b) if this value is positive, and otherwise 0. The use of the ppmi is based on the assumption that only the fact that words occur together more often than expected is interesting, while lower probabilities are more likely to be based on coincidence or in any case do not say anything interesting about word pairs.\n",
    "\n",
    "Another problem with our naive approach was that the vectors are extremely long. The vectors not only need a lot of storage space. Many of the dimensions also contain little or only redundant information. This problem can be solved by using a common dimension reduction process. Below we will use Singular Value Decomposition for this and then use the most important 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def makeCV_SVD(words,sentences, window = 2, minfreq = 10, size =256):\n",
    "\n",
    "    freq = Counter()\n",
    "    for s in sentences:\n",
    "        freq.update(s)\n",
    "    \n",
    "    context_words = [w for w,f in freq.items() if f > minfreq]\n",
    "    for w in words:\n",
    "        if w not in context_words:\n",
    "            context_words.append(w)\n",
    "    dim = len(context_words)\n",
    "    cw2nr = {}\n",
    "    for i in range(dim):\n",
    "        cw = context_words[i]\n",
    "        cw2nr[cw] = i\n",
    "              \n",
    "    w2nr = {}\n",
    "    for i in range(len(words)):\n",
    "        w = words[i]\n",
    "        w2nr[w] = i\n",
    "    \n",
    "    \n",
    "    matrix = sparse.lil_matrix((len(words),dim))\n",
    "    \n",
    "    n = 0\n",
    "    for s in sentences:\n",
    "        n+=1\n",
    "        i_s = [cw2nr.get(w,-1) for w in s]\n",
    "        for i in range(len(s)):\n",
    "            w = s[i]\n",
    "            if w in words:\n",
    "                i_w = w2nr[w]\n",
    "                for j in range(max(0,i-window),min(i+window+1,len(s))):\n",
    "                    if i != j:\n",
    "                        i_cw = i_s[j]\n",
    "                        if i_cw > 0:\n",
    "                            matrix[i_w,i_cw] += 1\n",
    "     \n",
    "    #up to here nothing new\n",
    "    N = matrix.sum()\n",
    "    \n",
    "    #Let us  get the probabilities for each word:\n",
    "    freq_w = matrix.sum(axis = 1)\n",
    "    freq_w = np.array(freq_w.T)[0]\n",
    "    prob_w = np.array(freq_w) / N\n",
    "    \n",
    "    #Let us  get the probabilities for each context word:\n",
    "    freq_cw = matrix.sum(axis = 0)\n",
    "    freq_cw = np.array(freq_cw)[0]\n",
    "    prob_cw = np.array(freq_cw) / N\n",
    "     \n",
    "    (rows,cols) = matrix.nonzero() #Returns a tuple of arrays (row,col) containing the indices of the non-zero elements of the matrix.\n",
    "    for i_w,i_cw in zip(rows,cols):\n",
    "        p = matrix[i_w,i_cw]/N\n",
    "        p_w = prob_w[i_w]\n",
    "        p_cw = prob_cw[i_cw]\n",
    "        ppmi = max(0,math.log(p/(p_w * p_cw) )) \n",
    "        matrix[i_w,i_cw] = ppmi\n",
    "\n",
    "\n",
    "    #We cannot be completely sure, that for every word we found at least some positive pmi-values.\n",
    "    #The frequent neighbors of a word eventaully are not in the set of context words\n",
    "    #A row with only zeros, will cause a problem for the SVD\n",
    "    for i in range(len(words)):\n",
    "        if np.sum(matrix[i]) == 0:\n",
    "            print(\"Empty row for:\",words[i])\n",
    "            print(\"Please remove this word from the wordlist.\")\n",
    "        \n",
    "    svd = TruncatedSVD(n_components=size)\n",
    "    svd.fit(matrix)\n",
    "    matrix = svd.transform(matrix)\n",
    "    wordvectors = {}\n",
    "    for w,i_w in w2nr.items():\n",
    "        v = matrix[i_w] \n",
    "        wordvectors[w] = v/mag(v)\n",
    "    return wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_SVD = makeCV_SVD(vocabulary,sentences,window=2, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7656588064751748, 'Saal'),\n",
       " (0.7207829276642457, 'Wohnhaus'),\n",
       " (0.7207142783164258, 'Grundstück'),\n",
       " (0.7152888403695544, 'Schlosspark'),\n",
       " (0.7135164388009677, 'Friedhof'),\n",
       " (0.7116116278337536, 'Brunnen'),\n",
       " (0.7098123982917997, 'Haus'),\n",
       " (0.7097884871703362, 'Ensemble'),\n",
       " (0.7015669408292721, 'Pavillon'),\n",
       " (0.6983357459655675, 'Gebäudekomplex')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Garten',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.800376297961646, 'denken'),\n",
       " (0.7565373133084523, 'wenden'),\n",
       " (0.7562864699024485, 'erklären'),\n",
       " (0.7491746524505113, 'untersuchen'),\n",
       " (0.7422653877970952, 'tun'),\n",
       " (0.7410471936873776, 'erinnern'),\n",
       " (0.7382061553935914, 'bezeichnen'),\n",
       " (0.7373671888102061, 'sprechen'),\n",
       " (0.7343977368084303, 'verstehen'),\n",
       " (0.7339052023461071, 'handeln')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('betrachten',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9162071217833722, 'Ungarn'),\n",
       " (0.8816609385956848, 'Frankreich'),\n",
       " (0.8716921487018372, 'Norwegen'),\n",
       " (0.8706114254949588, 'Finnland'),\n",
       " (0.8667800502642821, 'Polen'),\n",
       " (0.8657185236417015, 'Brasilien'),\n",
       " (0.8651073124270825, 'Italien'),\n",
       " (0.8644199260561896, 'Dänemark'),\n",
       " (0.8629187503645185, 'Rumänien'),\n",
       " (0.8627403799360186, 'Spanien')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Schweden',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7858793030972667, 'Forest'),\n",
       " (0.7723566700190202, 'Valley'),\n",
       " (0.7615404010928544, 'Bay'),\n",
       " (0.760159925800872, 'Airport'),\n",
       " (0.75384902217407, 'Avenue'),\n",
       " (0.752376341608628, 'Castle'),\n",
       " (0.734140734019549, 'River'),\n",
       " (0.7339341247914842, 'Manhattan'),\n",
       " (0.7338995440774598, 'Central'),\n",
       " (0.7338595472828965, 'Bridge')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Park',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that park is not interpreted as a synonym for garden, but rather is perceived as part of English place names. The context 'English words' has apparently become more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5621115517023889"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(testdata,vectors_SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the last model from the notebook DistrSem1.ipynb and download the small set of\n",
    "synonyms and non-synonym word pairs STW German Synonyms from http://textmining.\n",
    "wp.hs-hannover.de/datasets.html. Rank the words form this dataset according to their\n",
    "predicted similarity.\n",
    "1. Evaluate the ranking using AUC\n",
    "2. Investigate whether and how the result depends on the following parameters. Test at\n",
    "least 2 parameters, better 3 or all 4.\n",
    "a) Window size\n",
    "b) Corpus size (i.e. use only a part of the corpus)\n",
    "c) Lower and upper frequency bound of words to be included as context words.\n",
    "d) Number of dimensions\n",
    "You do not have to test all combinations. Just test each of the parameters independently\n",
    "while fixing the other parameters to a reasonable value. Note that you have to change\n",
    "frequency ranges for word inclusion if you reduce the corpus size!\n",
    "Finally, if you are learning German, just for fun, you could try to rank the word pairs yourself\n",
    "(not looking at the labels) and compute the AUC of your personal ranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization was apparently successful:  the correlation has become significantly larger. Bullinaria and Levy (2007), Mohammad and Hirst (2012), Bullinaria and Levy (2012) and Kiela and Clark (2014) provide overviews of the combinations of parameters, training quantities, etc. that lead to optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb#ch0000043?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m stw:\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb#ch0000043?line=8'>9</a>\u001b[0m     w1,w2,sim \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb#ch0000043?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m w1 \u001b[39min\u001b[39;00m vocabulary \u001b[39mand\u001b[39;00m w2 \u001b[39min\u001b[39;00m vocabulary:\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb#ch0000043?line=10'>11</a>\u001b[0m         testdata_stw\u001b[39m.\u001b[39mappend((w1,w2,sim))\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb#ch0000043?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_bin\u001b[39m(data,vectors):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "stw = codecs.open('STW-Syn.txt','r','utf8')\n",
    "\n",
    "\n",
    "testdata_stw = []\n",
    "for line in stw:\n",
    "    w1,w2,sim = line.strip().split('\\t')\n",
    "    if w1 in vocabulary and w2 in vocabulary:\n",
    "        testdata_stw.append((w1,w2,sim))\n",
    "    \n",
    "def evaluate_bin(data,vectors):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for v,w,sim in data:\n",
    "        pred = vectors[v].dot(vectors[w])\n",
    "        labels.append(sim)\n",
    "        predictions.append(pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, predictions, pos_label='+')\n",
    "    return metrics.auc(fpr, tpr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5695592286501377"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bin(testdata_stw,vectors_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import math\n",
    "\n",
    "def makeCV_SVD(words,sentences, window = 2, minfreq = 10, size =256):\n",
    "    N = 0\n",
    "    freq = Counter()\n",
    "    for s in sentences:\n",
    "        N += len(s)\n",
    "        freq.update(s)\n",
    "    \n",
    "    context_words = [w for w,f in freq.items() if f > minfreq]\n",
    "    for w in words:\n",
    "        if w not in context_words:\n",
    "            context_words.append(w)\n",
    "    dim = len(context_words)\n",
    "    cw2nr = {}\n",
    "    for i in range(dim):\n",
    "        cw = context_words[i]\n",
    "        cw2nr[cw] = i\n",
    "              \n",
    "    w2nr = {}\n",
    "    for i in range(len(words)):\n",
    "        w = words[i]\n",
    "        w2nr[w] = i\n",
    "    \n",
    "    \n",
    "    matrix = sparse.lil_matrix((len(words),dim))\n",
    "    n  =  0 \n",
    "    for s in sentences : \n",
    "        n += 1 \n",
    "        i_s = [cw2nr . get (w ,-1) for w in s] \n",
    "        for  i  in  range (len(s)): \n",
    "            w  =  s[i] \n",
    "            if w in words : \n",
    "                i_w = w2nr[w] \n",
    "                for j in range(max(0,i-window),min(i+window+1,len(s))):\n",
    "                    if i != j:\n",
    "                        i_cw = i_s[j]\n",
    "                        if i_cw > 0:\n",
    "                            matrix[i_w,i_cw] += 1\n",
    "     \n",
    "    \n",
    "    probabilities = []\n",
    "    for cw in context_words:\n",
    "        probabilities.append(freq[cw]/N)\n",
    "        \n",
    "    N2 = matrix.sum()/2\n",
    "    (rows,cols) = matrix.nonzero()\n",
    "    for i_w,i_cw in zip(rows,cols):\n",
    "        p_w = probabilities[i_w]\n",
    "        p_cw = probabilities[i_cw]\n",
    "        p = matrix[i_w,i_cw]/N2\n",
    "        ppmi = max(0,math.log(p/(p_w * p_cw * 2 * window) ))\n",
    "        matrix[i_w,i_cw] = ppmi\n",
    "\n",
    "    svd = TruncatedSVD(n_components=size)\n",
    "    svd.fit(matrix)\n",
    "    matrix = svd.transform(matrix)\n",
    "    wordvectors = {}\n",
    "    for w,i_w in w2nr.items():\n",
    "        v = matrix[i_w] \n",
    "        wordvectors[w] = v/mag(v)\n",
    "    return wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28439/3406290792.py:63: RuntimeWarning: invalid value encountered in true_divide\n",
      "  wordvectors[w] = v/mag(v)\n"
     ]
    }
   ],
   "source": [
    "vectors_SVD = makeCV_SVD(vocabulary,sentences,window=3,size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.8995127802671076, 'Mensch'), (0.8968436141268652, 'Menschheit'), (0.8922717188782779, 'Wirklichkeit'), (0.886289398182873, 'Seele'), (0.884582083665708, 'Alles'), (0.8799719982434838, 'Hoffnung'), (0.8798842901305821, 'Freiheit'), (0.8780358138034768, 'Herzen'), (0.8722102724041599, 'wirklich'), (0.8718914424311429, 'Inhalt')], [(0.9587682422089914, 'Brasilien'), (0.949968023470248, 'Finnland'), (0.9437857649002306, 'Portugal'), (0.9422672286566536, 'Norwegen'), (0.940137910673187, 'Südafrika'), (0.9393354308197659, 'Belgien'), (0.9369897591788723, 'Neuseeland'), (0.9332148559484487, 'Russland'), (0.9324117070340221, 'Rumänien'), (0.9322421387382575, 'Südkorea')], [(0.9016053402777532, 'Street'), (0.8954645259592968, 'Castle'), (0.8815114307156195, 'Station'), (0.8812594734162735, 'Point'), (0.8761696335065321, 'Town'), (0.874540305265088, 'Lake'), (0.872313829681639, 'Fort'), (0.8710674608574857, 'District'), (0.8704109238462485, 'River'), (0.8672713522034854, 'Beach')]]\n"
     ]
    }
   ],
   "source": [
    "wordlist = ['Guten', 'Schweden', 'Park']\n",
    "out = []\n",
    "for w in wordlist:\n",
    "    out.append(most_similar(w,vectors_SVD,10))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4554505431937267"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(testdata,vectors_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8849862258953168"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bin(testdata_stw,vectors_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'makeCV_SVD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb Cell 52'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/Farjad_Ahmed/Masters/NLP/Tutorials/Tutorial_5/Notebook-20220524/DistrSem1.ipynb#ch0000062?line=0'>1</a>\u001b[0m vectors_SVD \u001b[39m=\u001b[39m makeCV_SVD(vocabulary,sentences,window\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'makeCV_SVD' is not defined"
     ]
    }
   ],
   "source": [
    "vectors_SVD = makeCV_SVD(vocabulary,sentences,window=10,size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28439/637546162.py:62: RuntimeWarning: invalid value encountered in true_divide\n",
      "  wordvectors[w] = v/mag(v)\n"
     ]
    }
   ],
   "source": [
    "vectors_SVD = makeCV_SVD(vocabulary,sentences,window=4,size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.8999490135870389, 'Welt'), (0.8949092421143433, 'uns'), (0.894479972305323, 'Alles'), (0.8944054820737201, 'nichts'), (0.8940397605645949, 'alles'), (0.8905213819661983, 'niemals'), (0.8870250218866869, 'Mensch'), (0.8858753261855015, 'Inhalt'), (0.8854636456730761, 'Glück'), (0.8851294649507867, 'wirklich')], [(0.9549811439444973, 'Rumänien'), (0.9451361527044202, 'Brasilien'), (0.9433453065498792, 'Norwegen'), (0.9402278560646061, 'Finnland'), (0.9394951882796146, 'Portugal'), (0.931587130611133, 'Russland'), (0.9284639681128024, 'Belgien'), (0.9273848256719881, 'Argentinien'), (0.9197571526507095, 'Südkorea'), (0.9145677981354409, 'Großbritannien')], [(0.9142662476261989, 'Street'), (0.8813374309829919, 'Parks'), (0.8727515588687252, 'Castle'), (0.8727378352089697, 'Point'), (0.8688774618365562, 'Inn'), (0.8679456263438123, 'Station'), (0.8670722098173055, 'District'), (0.8655152607021549, 'Lake'), (0.863141903137295, 'Fort'), (0.8598394591878822, 'Forest')]]\n"
     ]
    }
   ],
   "source": [
    "wordlist = ['Guten', 'Schweden', 'Park']\n",
    "out = []\n",
    "for w in wordlist:\n",
    "    out.append(most_similar(w,vectors_SVD,10))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48280165647560497"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(testdata,vectors_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226354453627181"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bin(testdata_stw,vectors_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28439/637546162.py:62: RuntimeWarning: invalid value encountered in true_divide\n",
      "  wordvectors[w] = v/mag(v)\n"
     ]
    }
   ],
   "source": [
    "vectors_SVD = makeCV_SVD(vocabulary,sentences,window=3,size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.67387563794858, 'uns'), (0.6733058057944523, 'Mensch'), (0.665616925412969, 'Ursprung'), (0.6639226362338467, 'Herr'), (0.662839887571585, 'Glück'), (0.658911864463208, 'Inhalt'), (0.6561600636162114, 'Menschheit'), (0.6555802350722184, 'Hoffnung'), (0.6549472626623396, 'Gedanken'), (0.6470261659284583, 'Wort')], [(0.8478660585430271, 'Finnland'), (0.824076476069361, 'Rumänien'), (0.8106148313742586, 'Brasilien'), (0.8092346036916248, 'Argentinien'), (0.8070310592588065, 'Norwegen'), (0.8045919466919694, 'Belgien'), (0.7990961840516251, 'Russland'), (0.7949284583252829, 'Südafrika'), (0.7934430475948007, 'Großbritannien'), (0.7922372468641826, 'Tschechien')], [(0.7834402591118318, 'Street'), (0.7772969379691733, 'Lake'), (0.7616308770775474, 'River'), (0.7573451722292569, 'Valley'), (0.7566997972374241, 'District'), (0.7554807470219042, 'Fort'), (0.7502335612164226, 'Station'), (0.7492704059872841, 'Forest'), (0.7383029323895508, 'Bridge'), (0.7344059403276525, 'Parks')]]\n"
     ]
    }
   ],
   "source": [
    "wordlist = ['Guten', 'Schweden', 'Park']\n",
    "out = []\n",
    "for w in wordlist:\n",
    "    out.append(most_similar(w,vectors_SVD,10))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9070247933884298"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bin(testdata_stw,vectors_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9070247933884298"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bin(testdata_stw,vectors_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubenstein, H., & Goodenough, J. B. (1965). Contextual Correlates of Synonymy. _Commun. ACM_, 8(10), 627–633. \n",
    "\n",
    "Ruge, G., & Schwarz, C. (1990). Linguistically based term associations—A new semantic component for a hyperterm system. _Proceedings of 1st International ISKO-Confercence_, Darmstadt, 88-96.\n",
    "\n",
    "Crouch, C. J. (1990). An approach to the automatic construction of global thesauri. _Information Processing & Management_, 5, 629–640.\n",
    "\n",
    "Crouch, C. J., & Yang, B. (1992). Experiments in automatic statistical thesaurus construction. In _Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval_ (pp. 77-88). ACM.\n",
    "\n",
    "Grefenstette, G. (1992). Use of syntactic context to produce term association lists for text  retrieval. 89–97.\n",
    "\n",
    "Karlgren, J., & Sahlgren, M. (2001). From Words to Understanding. In _Foundations of Real-World Intelligence (S. 294–308)_. CSLI Publications.\n",
    "\n",
    "Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations from word co-occurrence statistics: A computational study. _Behavior research methods_, 39(3), 510-526.\n",
    "\n",
    "Mohammad, S. M., & Hirst, G. (2012). Distributional measures of semantic distance: A survey. _arXiv preprint_ arXiv:1203.1858.\n",
    "\n",
    "Bullinaria, J. A., & Levy, J. P. (2012). Extracting Semantic Representations from Word Co-occurrence Statistics: Stop-lists, Stemming and SVD. _Behaviour Research Methods_, 44(3), 890–907.\n",
    "\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In _Advances in neural information processing systems_ (pp. 3111-3119).\n",
    "\n",
    "Kiela, D., & Clark, S. (2014). A Systematic Study of Semantic Vector Space Model Parameters. In _2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)_.\n",
    "\n",
    "Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving distributional similarity with lessons learned from word embeddings. _Transactions of the Association for Computational Linguistics_, 3, 211-225."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
